# 微服务之服务发现 #

假装大家都已经知道什么是微服务，那么我们先谈谈微服务架构中的一个重要组件——服务发现。


微服务架构中用于服务发现的主要有Consul、zookeeper、etcd、euerka。我们先做一个对比如下：

![](https://github.com/Hanzhiwei210521/loading/blob/master/image/image005.png)

详细分析请见：<https://luyiisme.github.io/2017/04/22/spring-cloud-service-discovery-products/>

### zookeeper ###

Zookeeper是著名Hadooop的一个子项目，旨在解决大规模分布式应用场景下，服务协调同步的问题；它可以为同在一个分布式系统中的其他服务提供：统一命名服务、配置管理、集群管理、分布式锁服务等功能。

zookeeper数据结构如下：

![](https://github.com/Hanzhiwei210521/loading/blob/master/image/image006.jpg)

* 统一命名服务（Name Service）

分布式应用中，通常需要有一套完整的命名规则，既能够产生唯一的名称又便于人识别和记住，通常情况下用树形的名称结构是一个理想的选择，树形的名称结构是一个有层次的目录结构，既对人友好又不会重复。说到这里你可能想到了JNDI，没错Zookeeper的Name Service与JNDI能够完成的功能是差不多的，它们都是将有层次的目录结构关联到一定资源上，但是Zookeeper的Name Service更加是广泛意义上的关联，也许你并不需要将名称关联到特定资源上，你可能只需要一个不会重复名称，就像数据库中产生一个唯一的数字主键一样。Name Service 已经是 Zookeeper 内置的功能，你只要调用 Zookeeper 的 API 就能实现。如调用 create 接口就可以很容易创建一个目录节点。

这么一段艰难晦涩的文字，其实本人并没有理解透彻，不过有一点需要补充，命名服务提供注册、注销和查看命名等接口，so，zookeeper作为注册中心，应该是通过该功能实现的吧。

* 配置管理（Configuration Management）

配置的管理在分布式应用环境中很常见，例如web集群都需要连接同一台redis、mysql、mq等，这些连接信息如果放在各个web服务器上，那维护起来就相当麻烦，而且容易出错。这时就需要一个统一配置管理。

像这样的配置信息完全可以交给Zookeeper来管理，将配置信息保存在Zookeeper的某个目录节点中，然后将所有需要修改的应用机器监控配置信息的状态，一旦配置信息发生变化，每台应用机器就会收到Zookeeper的通知，然后从Zookeeper获取新的配置信息应用的系统中。另外，我们还用过mysql做配置管理，将连接信息通过key-value存入MySQL的参数表中，然后web启动时先连接MySQL，读取参数表，然后获取其他连接信息，这样的弊端是每一台web都要存一个MySQL的连接信息。还有一种是基于spring cloud框架的统一配置管理，可以用git或svn实现。

配置管理结构图，如下：

![](https://github.com/Hanzhiwei210521/loading/blob/master/image/image007.png)

* 集群管理（Group Membership）

Zookeeper 能够很容易的实现集群管理的功能，如有多台 Server 组成一个服务集群，那么必须要一个“总管”知道当前集群中每台机器的服务状态，一旦有机器不能提供服务，集群中其它集群必须知道，从而做出调整重新分配服务策略。同样当增加集群的服务能力时，就会增加一台或多台 Server，同样也必须让“总管”知道。

Zookeeper 不仅能够帮你维护当前的集群中机器的服务状态，而且能够帮你选出一个“总管”，让这个总管来管理集群，这就是 Zookeeper 的另一个功能 Leader Election。

* 共享锁（Locks）

共享锁在同一个进程中很容易实现，但是在跨进程或者在不同 Server 之间就不好实现了。Zookeeper 却很容易实现这个功能，实现方式也是需要获得锁的 Server 创建一个 EPHEMERAL_SEQUENTIAL 目录节点，然后调用 getChildren方法获取当前的目录节点列表中最小的目录节点是不是就是自己创建的目录节点，如果正是自己创建的，那么它就获得了这个锁，如果不是那么它就调用 exists(String path, boolean watch) 方法并监控 Zookeeper 上目录节点列表的变化，一直到自己创建的节点是列表中最小编号的目录节点，从而获得锁，释放锁很简单，只要删除前面它自己所创建的目录节点就行了。

文章参考自： <https://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/>

## Eureka ##

Eureka是Netiflix开发的服务发现组件，本身是一个基于REST的服务。Spring Cloud将它集成在其子项目spring-cloud-netflix中，以实现Sping Cloud的服务发现功能。

Eureka的Github: <https://github.com/Netflix/Eureka>

Eureka架构

![](https://github.com/Hanzhiwei210521/loading/blob/master/image/image008.png)

以上是来自Eureka官方的架构图，大致描述了Eureka集群的工作过程。途中包含的组件非常多，可能比较难以理解，我们用通俗易懂的语言解释一下：

* Application Service 服务提供者，Application Client 服务消费者；
* Make Remote Call 可以简单理解为调用RESTful API；

由图可知，Eureka包含两个组件：Eureka Server和Eureka Client,他们的作用如下：

* Eureka Client是一个Java客户端，用于简化与Eureka Server的交互；
* Eureka Server提供服务发现的能力，各个微服务启动时，会通过Eureka Client向Eureka Server进行注册自己的信息，Eureka会存储该服务的信息；
* 微服务启动后，会周期性的向Eureka Server发送心跳（默认周期为30秒）以续约自己的信息。如果Eureka Server 在一定时间内没有接收到某个服务节点的心跳，Eureka Server将会注销该微服务节点（默认90秒）；
* 每个Eureka Server同时也是Eureka Client，多个Eureka Server之间通过复制的方式完成服务注册表的同步；
* Eureka Client会缓存Eureka Server中的信息。即使所有的Eureka Server节点都宕掉，服务消费者依然可以使用缓存中的信息找到服务提供者。

综上，Eureka通过心跳检测、健康检查和客户端缓存机制，提高了系统的灵活性，可伸缩和可用性。

## Zookeeper在服务发现上的表现 ##

在分布式系统领域有个著名的CAP定理（C-数据一致性；A-服务可用性；P-服务对网络分区故障的容错性，以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择；这三个特性在任何分布式系统中不能同时满足，最多同时满足两个）；Zookeeper是个CP的，即任何时刻对Zookeeper的访问请求都能得到一致性的数据结果，同时系统对网络分割具备容错性；但是他不能保证每次服务请求的而可用性（注：也就是在极端环境下，Zookeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果）。但是别忘了，Zookeeper是分布式协调服务，它的职责是保证数据（注：配置数据，状态数据）在其管辖下的所有服务之间保持同步、一致；所以就不难理解为什么Zookeeper被设计成CP而不是AP特性的了，如果是AP的，那么将会带来恐怖的后果（注：Zookeeper就像交叉路口的信号灯一样，你能想像在交通要道突然信号灯失灵的情况么）。而且，作为Zookeeper的核心算法Zab，就是解决了分布式系统下数据如何在多个服务之间保持同步问题的。

作为一个分布式系统服务，Zookeeper非常好，但是对于Service发现服务来说就不合适了；因为对于Service发现服务来说就算是返回了包含不实的信息的结果也比什么都不返回要好；再者，对于Service发现服务而言，宁可返回某服务5分钟之前在哪几个服务器上的可用的信息，也不能因为暂时的网络故障而找不到可用的服务器，而不返回任何结果。所以说，用Zookeeper来做Service发现服务坑定是错误的，如果你这么用就惨了！

更何况，如果被用作Service发现服务，Zookeeper本身没有正确的处理网络分割的问题；而在云端，网络分割问题跟其他类型的故障一样的确会发生；所以最好提前对这个问题做好100%的准备。就像Jepsen在Zookeeper网站上发布的博客中所说：在Zookeeper中，如果在啊同一个网络分区的节点数达不到Zookeeper选取Leader节点的“法定人数”时，他们就会从Zookeeper中断开，当然同时也不能提供Service发现服务了。

如果给ZooKeeper加上客户端缓存（注：给ZooKeeper节点配上本地缓存）或者其他类似技术的话可以缓解ZooKeeper因为网络故障造成节点同步信息错误的问题。Pinterest与Airbnb公司就使用了这个方法来防止ZooKeeper故障发生。这种方式可以从表面上解决这个问题，具体地说，当部分或者所有节点跟ZooKeeper断开的情况下，每个节点还可以从本地缓存中获取到数据；但是，即便如此，ZooKeeper下所有节点不可能保证任何时候都能缓存所有的服务注册信息。如果ZooKeeper下所有节点都断开了，或者集群中出现了网络分割的故障（注：由于交换机故障导致交换机底下的子网间不能互访）；那么ZooKeeper会将它们都从自己管理范围中剔除出去，外界就不能访问到这些节点了，即便这些节点本身是“健康”的，可以正常提供服务的；所以导致到达这些节点的服务请求被丢失了。（注：这也是为什么ZooKeeper不满足CAP中A的原因）

更深层次的原因是，ZooKeeper是按照CP原则构建的，也就是说它能保证每个节点的数据保持一致，而为ZooKeeper加上缓存的做法的目的是为了让ZooKeeper变得更加可靠（available）；但是，ZooKeeper设计的本意是保持节点的数据一致，也就是CP。所以，这样一来，你可能既得不到一个数据一致的（CP）也得不到一个高可用的（AP）的Service发现服务了；因为，这相当于你在一个已有的CP系统上强制栓了一个AP的系统，这在本质上就行不通的！一个Service发现服务应该从一开始就被设计成高可用的才行！

## Eureka在服务发现上的表现 ##

Eureka，它是一个开源的服务发现解决方案，由Netflix公司开发。Eureka由两个组件组成：Eureka服务器和Eureka客户端。Eureka服务器用作服务注册服务器。Eureka客户端是一个java客户端，用来简化与服务器的交互、作为轮询负载均衡器，并提供服务的故障切换支持。Eureka一开始就被设计成高可用与可伸缩的Service发现服务，这两个特点也是Netflix公司开发所有平台的两个特色。

在Eureka平台中，如果某台服务器宕机，Eureka不会有类似于ZooKeeper的选举leader的过程；客户端请求会自动切换到新的Eureka节点；当宕机的服务器重新恢复后，Eureka会再次将其纳入到服务器集群管理之中；而对于它来说，所有要做的无非是同步一些新的服务注册信息而已。所以，再也不用担心有“掉队”的服务器恢复以后，会从Eureka服务器集群中剔除出去的风险了。Eureka甚至被设计用来应付范围更广的网络分割故障，并实现“0”宕机维护需求。当网络分割故障发生时，每个Eureka节点，会持续的对外提供服务（注：ZooKeeper不会）：接收新的服务注册同时将它们提供给下游的服务发现请求。这样一来，就可以实现在同一个子网中（same side of partition），新发布的服务仍然可以被发现与访问。

但是，Eureka做到的不止这些。正常配置下，Eureka内置了心跳服务，用于淘汰一些“濒死”的服务器；如果在Eureka中注册的服务，它的“心跳”变得迟缓时，Eureka会将其整个剔除出管理范围（这点有点像ZooKeeper的做法）。这是个很好的功能，但是当网络分割故障发生时，这也是非常危险的；因为，那些因为网络问题（注：心跳慢被剔除了）而被剔除出去的服务器本身是很”健康“的，只是因为网络分割故障把Eureka集群分割成了独立的子网而不能互访而已。

幸运的是，Netflix考虑到了这个缺陷。如果Eureka服务节点在短时间里丢失了大量的心跳连接（注：可能发生了网络故障），那么这个Eureka节点会进入”自我保护模式“，同时保留那些“心跳死亡“的服务注册信息不过期。此时，这个Eureka节点对于新的服务还能提供注册服务，对于”死亡“的仍然保留，以防还有客户端向其发起请求。当网络故障恢复后，这个Eureka节点会退出”自我保护模式“。所以Eureka的哲学是，同时保留”好数据“与”坏数据“总比丢掉任何”好数据“要更好，所以这种模式在实践中非常有效。

最后，Eureka还有客户端缓存功能（注：Eureka分为客户端程序与服务器端程序两个部分，客户端程序负责向外提供注册与发现服务接口）。所以即便Eureka集群中所有节点都失效，或者发生网络分割故障导致客户端不能访问任何一台Eureka服务器；Eureka服务的消费者仍然可以通过Eureka客户端缓存来获取现有的服务注册信息。甚至最极端的环境下，所有正常的Eureka节点都不对请求产生相应，也没有更好的服务器解决方案来解决这种问题时；得益于Eureka的客户端缓存技术，消费者服务仍然可以通过Eureka客户端查询与获取注册服务信息，这点很重要。

Eureka的构架保证了它能够成为Service发现服务。它相对与ZooKeeper来说剔除了Leader节点的选取或者事务日志机制，这样做有利于减少使用者维护的难度也保证了Eureka的在运行时的健壮性。而且Eureka就是为发现服务所设计的，它有独立的客户端程序库，同时提供心跳服务、服务健康监测、自动发布服务与自动刷新缓存的功能。但是，如果使用ZooKeeper你必须自己来实现这些功能。Eureka的所有库都是开源的，所有人都能看到与使用这些源代码，这比那些只有一两个人能看或者维护的客户端库要好。

维护Eureka服务器也非常的简单，比如，切换一个节点只需要在现有EIP下移除一个现有的节点然后添加一个新的就行。Eureka提供了一个web-based的图形化的运维界面，在这个界面中可以查看Eureka所管理的注册服务的运行状态信息：是否健康，运行日志等。Eureka甚至提供了Restful-API接口，方便第三方程序集成Eureka的功能。

文章摘自： <http://dockone.io/article/78>

